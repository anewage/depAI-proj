{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cats_vs_dogs (/home/amir/.cache/huggingface/datasets/cats_vs_dogs/default/1.0.0/d4fe9cf31b294ed8639aa58f7d8ee13fe189011837038ed9a774fde19a911fcb)\n",
      "100%|██████████| 1/1 [00:00<00:00, 550.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTForImageClassification\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "class SegmentationModelOutputWrapper(torch.nn.Module):\n",
    "  def __init__(self, model): \n",
    "    super(SegmentationModelOutputWrapper, self).__init__()\n",
    "    self.model = model\n",
    "        \n",
    "  def forward(self, x):\n",
    "    return self.model(x)[\"out\"]\n",
    "\n",
    "\n",
    "class SemanticSegmentationTarget:\n",
    "  def __init__(self, category, mask):\n",
    "    self.category = category\n",
    "    self.mask = torch.from_numpy(mask)\n",
    "    if torch.cuda.is_available():\n",
    "      self.mask = self.mask.cuda()\n",
    "        \n",
    "  def __call__(self, model_output):\n",
    "    return (model_output[self.category, :, : ] * self.mask).sum()\n",
    "\n",
    "sem_classes = [\n",
    "  '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "  'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "  'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}\n",
    "desired_category = sem_class_to_idx[\"cat\"]\n",
    "\n",
    "dataset = load_dataset(\"cats_vs_dogs\", revision=\"main\")\n",
    "\n",
    "model = deeplabv3_resnet101(pretrained=True, progress=False)\n",
    "model = model.eval()\n",
    "model = SegmentationModelOutputWrapper(model)\n",
    "\n",
    "\n",
    "from torchvision.models import swin_v2_b\n",
    "my_transformer = ViTForImageClassification.from_pretrained('google/vit-large-patch32-384')\n",
    "# transformer_target_layers = [my_transformer.vit.encoder.layer[-1].layernorm_before]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(image):\n",
    "  inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "  outputs = my_transformer(**inputs)\n",
    "  logits = outputs.logits\n",
    "  predicted_class_idx = logits.argmax(-1).item()\n",
    "  return predicted_class_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(image):\n",
    "  rgb_img = np.float32(image) / 255\n",
    "  image_tensor = preprocess_image(rgb_img,\n",
    "                                  mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "  out = model(image_tensor)\n",
    "  \n",
    "  normalized_masks = torch.nn.functional.softmax(out, dim=1).cpu()\n",
    "  \n",
    "  # Image.fromarray(both_images).save(f'output/{index}_mask.jpg')\n",
    "  \n",
    "  desired_mask = normalized_masks[0, :, :, :].argmax(axis=0).detach().cpu().numpy()\n",
    "  # desired_mask_uint8 = 255 * np.uint8(desired_mask == desired_category)\n",
    "  # desired_mask_float = np.float32(desired_mask == desired_category)\n",
    "\n",
    "  return desired_mask\n",
    "  \n",
    "  #both_images = np.hstack((image, np.repeat(desired_mask_uint8[:, :, None], 3, axis=-1)))\n",
    "  #Image.fromarray(both_images).save(f'output/{index}_mask.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  1 ...\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print('Processing ', index, '...')\n",
    "image = np.array(dataset[\"train\"][\"image\"][index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input image size (224*224) doesn't match model (384*384).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted_label \u001b[39m=\u001b[39m get_label(image)\n\u001b[1;32m      2\u001b[0m true_label \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m][index]\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrue label: \u001b[39m\u001b[39m'\u001b[39m, true_label, \u001b[39m'\u001b[39m\u001b[39m, Predicted: \u001b[39m\u001b[39m'\u001b[39m, predicted_label)\n",
      "Cell \u001b[0;32mIn[105], line 3\u001b[0m, in \u001b[0;36mget_label\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_label\u001b[39m(image):\n\u001b[1;32m      2\u001b[0m   inputs \u001b[39m=\u001b[39m feature_extractor(images\u001b[39m=\u001b[39mimage, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m   outputs \u001b[39m=\u001b[39m my_transformer(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m      4\u001b[0m   logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m      5\u001b[0m   predicted_class_idx \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:797\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[39m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    795\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 797\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvit(\n\u001b[1;32m    798\u001b[0m     pixel_values,\n\u001b[1;32m    799\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    800\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    801\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    802\u001b[0m     interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding,\n\u001b[1;32m    803\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    804\u001b[0m )\n\u001b[1;32m    806\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    808\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output[:, \u001b[39m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:583\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m pixel_values\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m expected_dtype:\n\u001b[1;32m    581\u001b[0m     pixel_values \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mto(expected_dtype)\n\u001b[0;32m--> 583\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    584\u001b[0m     pixel_values, bool_masked_pos\u001b[39m=\u001b[39;49mbool_masked_pos, interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding\n\u001b[1;32m    585\u001b[0m )\n\u001b[1;32m    587\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    588\u001b[0m     embedding_output,\n\u001b[1;32m    589\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:122\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    117\u001b[0m     pixel_values: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    118\u001b[0m     bool_masked_pos: Optional[torch\u001b[39m.\u001b[39mBoolTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m     interpolate_pos_encoding: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    121\u001b[0m     batch_size, num_channels, height, width \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 122\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding)\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m bool_masked_pos \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         seq_length \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:177\u001b[0m, in \u001b[0;36mViTPatchEmbeddings.forward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interpolate_pos_encoding:\n\u001b[1;32m    176\u001b[0m     \u001b[39mif\u001b[39;00m height \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m width \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 177\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    178\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image size (\u001b[39m\u001b[39m{\u001b[39;00mheight\u001b[39m}\u001b[39;00m\u001b[39m*\u001b[39m\u001b[39m{\u001b[39;00mwidth\u001b[39m}\u001b[39;00m\u001b[39m) doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m*\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(pixel_values)\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    182\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mValueError\u001b[0m: Input image size (224*224) doesn't match model (384*384)."
     ]
    }
   ],
   "source": [
    "predicted_label = get_label(image)\n",
    "true_label = dataset[\"train\"][\"labels\"][index]\n",
    "print('True label: ', true_label, ', Predicted: ', predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEZCAAAAAAIoUe3AAAErElEQVR4nO3d0XajMAxFUXdW//+XMw9pm0DAcG1JYPmc55kCG5nQhLalEBERERERERERERERERERERERERHRLXvEbeorblMuPamCjmJorLehCjmOcbHWyy/gSEbF2rhS+R/Kd+8XWOx1FH3gRf29zsO75ATvUblvuWsD1+z17li5Y/3r+L+7e+25Sh4XLcFSurAqe+13RBdS9Yzu0W57LIr6Nu+7DA9PscMMHHxJ96nruWYdZL3vV16tnrWO7qkdN1oXp5G812Hj1z+7//a3cY5bO6z7Dr7eo3n/L19zGzlPVuMG2qTcXw2dJ6tltu44U8/azoZ0PMomuqDGn6xSHqeP4r4z9Sxgso43Y4M0wPtZ59qerrtP0rogrD+u0XwWBS3DgALepXX83jBfYAmlwYr4rCQNVkRgCYElBJZQFqyQz8KzYIUElhBYQkmwYh7fSYIVE1hCbVijPi/YGZMllAMraNJzYAUFlhBYQmAJgSUElhBYQmAJgSUElhBYQmAJNWLN+R4NkyUEllAOrKCH6xqx7vjon385JiuoJFgxk54EKyawhLJghazDLFghgSWUBitiHbZhzXlPmmeyIsqDFTDtebACSoTlP1pNWJNe3zNN1tC/uCdfYAmlwvJehy1Ys17fc02Wd7mwnGc+F5ZzYAmBJZQMy/eilQzLN7CEwBLKhuV60cqG5aqVDsuzFqx7P1DqOFoJJ8tPKyGWn1ZGLDetlFheWjmxnLSSYvloNWHd+97hmYdW1sly+fsgebEchiszlrlW7t9yZKyVerKsS45lO1rJsWxfE/P/2K+hVvbJKpZaE2DZLcVWrIHWYTEbrhkmq1hpNWONNVo2WpNMls2FaxosC66JsPq5Oi49Yz6H23OtnWqySumbrh7oMUertB/0dJNVSvtZnhKrVavr3nLYdVjaDnzOySptF/ppsVrWRd+3eCOvw1Lko594sop8sufGErU632kZfR0WSWDyySrS+QZL0AJL0AKrnNcCq5TTWr2fOyR4OXx2BoLJ+unMWQdLCKzfTowWWH8da4H16lALrLeOtMB670ALrEV1rV6swR6mOayqxWStqmmBta6iBdZH+1pgfbarBdZGe1pgbbXzcTVY221qgbXTlhZYQmDttTFa/d+upHkX/rM1DpNVaT0HYNVaaYFVbakFVr2FFlgHvd/Mg3XYiwusE/1qgXWmn+HiplSof7KyfWRRiWUoBJYQWEJgCYElBJYQWEJgCRlgzXNXymQJgSUElhBYQmAJgSUElpAF1jQ3WkyWEFhCYAmBJQSWEFhCYAmBJQSWEFhCYAmBJQSWEFhCYAmBJQSWEFhCYAmBJQSWEFhCJlizfBbGZAmBJQSWEFhCYAnZYE3ycshkCYElZIQ1xzpksoSssKYYLSZLyAxrhtGym6wJtAyXYX4trllCYAlZYqVfh0yWEFhCYAmBJQSWEFhCYAmBJWSKlf2ulMkSssVKPlrGk5Vbi2UoZI2VerSYLCFzrMyjZT9ZibVYhkIOWHlHy+XIrv2t55VDWuzY/r/b2/97Yy33zuVvLFV6be/Lc2sGWndczE771Kd1R6hSSvm+egc+uysVtw5SYAmBJfQfA/eakI+CLLMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=300x281>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_mask(image)\n",
    "Image.fromarray(255* np.uint8(mask == desired_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from pytorch_grad_cam import run_dff_on_image, GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Callable, Optional\n",
    "\n",
    "img_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "\"\"\" Model wrapper to return a tensor\"\"\"\n",
    "class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "\"\"\" Translate the category name to the category index.\n",
    "    Some models aren't trained on Imagenet but on even larger datasets,\n",
    "    so we can't just assume that 761 will always be remote-control.\n",
    "\n",
    "\"\"\"\n",
    "def category_name_to_index(model, category_name):\n",
    "    name_to_index = dict((v, k) for k, v in model.config.id2label.items())\n",
    "    return name_to_index[category_name]\n",
    "    \n",
    "\"\"\" Helper function to run GradCAM on an image and create a visualization.\n",
    "    (note to myself: this is probably useful enough to move into the package)\n",
    "    If several targets are passed in targets_for_gradcam,\n",
    "    e.g different categories,\n",
    "    a visualization for each of them will be created.\n",
    "    \n",
    "\"\"\"\n",
    "def run_grad_cam_on_image(model: torch.nn.Module,\n",
    "                          target_layer: torch.nn.Module,\n",
    "                          targets_for_gradcam: List[Callable],\n",
    "                          reshape_transform: Optional[Callable],\n",
    "                          input_tensor: torch.nn.Module=img_tensor,\n",
    "                          input_image: Image=image,\n",
    "                          method: Callable=GradCAM):\n",
    "    with method(model=HuggingfaceToTensorModelWrapper(model),\n",
    "                 target_layers=[target_layer],\n",
    "                 reshape_transform=reshape_transform) as cam:\n",
    "\n",
    "        # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n",
    "        repeated_tensor = input_tensor[None, :].repeat(len(targets_for_gradcam), 1, 1, 1)\n",
    "\n",
    "        batch_results = cam(input_tensor=repeated_tensor,\n",
    "                            targets=targets_for_gradcam)\n",
    "        results = []\n",
    "        for grayscale_cam in batch_results:\n",
    "            visualization = show_cam_on_image(np.float32(input_image)/255,\n",
    "                                              grayscale_cam,\n",
    "                                              use_rgb=True)\n",
    "            # Make it weight less in the notebook:\n",
    "            visualization = cv2.resize(visualization,\n",
    "                                       (visualization.shape[1]//2, visualization.shape[0]//2))\n",
    "            results.append(visualization)\n",
    "        return np.hstack(results)\n",
    "    \n",
    "    \n",
    "def print_top_categories(model, img_tensor, top_k=5):\n",
    "    logits = model(img_tensor.unsqueeze(0)).logits\n",
    "    indices = logits.cpu()[0, :].detach().numpy().argsort()[-top_k :][::-1]\n",
    "    for i in indices:\n",
    "        print(f\"Predicted class {i}: {model.config.id2label[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SegmentationModelOutputWrapper' object has no attribute 'vit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m targets_for_gradcam \u001b[39m=\u001b[39m [ClassifierOutputTarget(category_name_to_index(my_transformer, \u001b[39m\"\u001b[39m\u001b[39mEgyptian cat\u001b[39m\u001b[39m\"\u001b[39m)),\n\u001b[1;32m     10\u001b[0m                        ClassifierOutputTarget(category_name_to_index(my_transformer, \u001b[39m\"\u001b[39m\u001b[39mremote control, remote\u001b[39m\u001b[39m\"\u001b[39m))]\n\u001b[0;32m---> 11\u001b[0m target_layer_dff \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mvit\u001b[39m.\u001b[39mlayernorm\n\u001b[1;32m     12\u001b[0m target_layer_gradcam \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mvit\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlayer[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39moutput\n\u001b[1;32m     13\u001b[0m image_resized \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mresize((\u001b[39m384\u001b[39m, \u001b[39m384\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/dev/depAI-proj/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentationModelOutputWrapper' object has no attribute 'vit'"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "def reshape_transform_vit_huggingface(x):\n",
    "    activations = x[:, 1:, :]\n",
    "    activations = activations.view(activations.shape[0],\n",
    "                                   12, 12, activations.shape[2])\n",
    "    activations = activations.transpose(2, 3).transpose(1, 2)\n",
    "    return activations\n",
    "\n",
    "targets_for_gradcam = [ClassifierOutputTarget(category_name_to_index(my_transformer, \"Egyptian cat\")),\n",
    "                       ClassifierOutputTarget(category_name_to_index(my_transformer, \"remote control, remote\"))]\n",
    "target_layer_dff = model.vit.layernorm\n",
    "target_layer_gradcam = model.vit.encoder.layer[-2].output\n",
    "image_resized = image.resize((384, 384))\n",
    "tensor_resized = transforms.ToTensor()(image_resized)\n",
    "\n",
    "\n",
    "display(Image.fromarray(run_dff_on_image(model=model,\n",
    "                          target_layer=target_layer_dff,\n",
    "                          classifier=model.classifier,\n",
    "                          img_pil=image_resized,\n",
    "                          img_tensor=tensor_resized,\n",
    "                          reshape_transform=reshape_transform_vit_huggingface,\n",
    "                          n_components=4,\n",
    "                          top_k=2)))\n",
    "display(Image.fromarray(run_grad_cam_on_image(model=model,\n",
    "                      target_layer=target_layer_gradcam,\n",
    "                      targets_for_gradcam=targets_for_gradcam,\n",
    "                      input_tensor=tensor_resized,\n",
    "                      input_image=image_resized,\n",
    "                      reshape_transform=reshape_transform_vit_huggingface)))\n",
    "print_top_categories(model, tensor_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
